{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import date,datetime\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import nltk  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import textblob\n",
    "from textblob import TextBlob \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tweets from twitter search (works with '@' and '#' prefix)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def connect_to_twitter(twitter_auth_path):\n",
    "    with open(twitter_auth_path, 'r') as f:\n",
    "        twitter_auth = json.load(f)\n",
    "        APP_KEY = twitter_auth['APP_KEY'] # your app key\n",
    "        APP_SECRET = twitter_auth['APP_SECRET'] # your app secret\n",
    "        OAUTH_TOKEN = twitter_auth['OAUTH_TOKEN'] # your oauth token\n",
    "        OAUTH_TOKEN_SECRET = twitter_auth['OAUTH_TOKEN_SECRET'] # your oauth token secret\n",
    "    twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "    return twitter\n",
    "\n",
    "def get_tweet_data(tweet, tweet_data):        \n",
    "    tweet_data['text'] = tweet['full_text']\n",
    "    tweet_data['hashtags'] = get_hashtags_string(tweet)\n",
    "    tweet_data['tweet_id'] = tweet['id']\n",
    "    tweet_data['created_at'] = tweet['created_at']\n",
    "    tweet_data['retweet_count'] = tweet['retweet_count']\n",
    "    tweet_data['favorite_count'] = tweet['favorite_count']\n",
    "    tweet_data['in_reply_to_status_id'] = tweet['in_reply_to_status_id']\n",
    "    tweet_data['in_reply_to_screen_name'] = tweet['in_reply_to_screen_name']\n",
    "    tweet_data['lang'] = tweet['lang']\n",
    "    return tweet_data\n",
    "\n",
    "def get_tweet_author_data(tweet, tweet_data):\n",
    "    tweet_data['author'] = tweet['user']['screen_name']\n",
    "    tweet_data['account_created_at'] = tweet['user']['created_at']\n",
    "    tweet_data['author_description'] = tweet['user']['description']\n",
    "    tweet_data['author_id'] = tweet['user']['id']\n",
    "    tweet_data['author_location'] = tweet['user']['location']\n",
    "    tweet_data['author_statuses_count'] = tweet['user']['statuses_count']\n",
    "    tweet_data['author_followers_count'] = tweet['user']['followers_count']\n",
    "    tweet_data['author_friends_count'] = tweet['user']['friends_count']\n",
    "    tweet_data['author_favourites_count'] = tweet['user']['favourites_count']\n",
    "    tweet_data['author_listed_count'] = tweet['user']['listed_count']\n",
    "    return tweet_data\n",
    "\n",
    "def scraper_metadata(tweet_data, scrap_time, search_phrase):\n",
    "    tweet_data['scrap_time'] = scrap_time\n",
    "    tweet_data['file_tag'] = scrap_time[:19]\n",
    "    tweet_data['scrap_phrase'] = search_phrase\n",
    "    return tweet_data\n",
    "\n",
    "def get_hashtags_string(tweet):\n",
    "    hashtags_string = ''\n",
    "    for nr in range(len(tweet['entities']['hashtags'])):\n",
    "        hashtags_string += tweet['entities']['hashtags'][nr]['text'] + ' '\n",
    "    if not hashtags_string:\n",
    "        hashtags_string = 'NO_HASHTAGS'\n",
    "    return hashtags_string\n",
    "\n",
    "def get_tweets_from_search(twitter, max_attempts, max_tweets_to_get, search_phrase):\n",
    "    tweets_data = []\n",
    "    for attempt_nr in range(0, max_attempts):\n",
    "        if(max_tweets_to_get < len(tweets_data)):\n",
    "            break\n",
    "\n",
    "        if(0 == attempt_nr):\n",
    "            search_results = twitter.search(q=search_phrase, result_type='mixed',  count='100', lang='en', tweet_mode='extended')\n",
    "        else:\n",
    "            search_results = twitter.search(q=search_phrase, result_type='mixed', include_entities='true', max_id=next_max_id, count='100', lang='en', tweet_mode='extended')\n",
    "\n",
    "        for tweet_data in search_results['statuses']:\n",
    "            tweets_data.append(tweet_data)\n",
    "        print('Loop: {} finished. Tweets gathered sum: {}.'.format(attempt_nr+1, len(tweets_data)))\n",
    "\n",
    "        try:\n",
    "            metadata = search_results['search_metadata']['next_results']\n",
    "            next_max_id = metadata.split('max_id=')[1]\n",
    "            next_max_id = next_max_id.split('&')[0]\n",
    "        except:\n",
    "            break\n",
    "    return tweets_data\n",
    "\n",
    "def return_as_df(all_tweets_list):\n",
    "    tweet_df = pd.DataFrame(all_tweets_list)\n",
    "    if tweet_df.shape[0] > 0:\n",
    "        display(tweet_df.text.head(5), tweet_df.shape)\n",
    "    return tweet_df\n",
    "\n",
    "def save_tweets_as_CSV(save_path, tweet_df):\n",
    "    tweet_nr = len(tweet_df)\n",
    "    tweet_df.to_csv(save_path, sep='\\t', encoding='utf-8', index=False)\n",
    "    print('{} tweets saved to {}'.format(tweet_nr, save_path))\n",
    "\n",
    "def get_tweets_by_search_phrase(s, max_tweets_to_get, max_attempts, twitter_auth_path, SAVE_DIR=''):\n",
    "    scrap_time = datetime.now().isoformat()\n",
    "    twitter = connect_to_twitter(twitter_auth_path)\n",
    "    tweets_data = get_tweets_from_search(twitter, max_attempts, max_tweets_to_get, search_phrase)\n",
    "\n",
    "    extracted_tweets_data = []\n",
    "    for tweet in tweets_data:\n",
    "        tweet_data = {}\n",
    "        tweet_data = get_tweet_data(tweet, tweet_data)\n",
    "        tweet_data = get_tweet_author_data(tweet, tweet_data)\n",
    "        tweet_data = scraper_metadata(tweet_data, scrap_time, search_phrase)\n",
    "        extracted_tweets_data.append(tweet_data)\n",
    "    \n",
    "    tweet_df = return_as_df(extracted_tweets_data)\n",
    "    time_tag = ''.join([l for l in scrap_time[:19] if l.isdigit()])\n",
    "    save_path = SAVE_DIR + ''.join(search_phrase.split(' ')) +'_' + time_tag + '_tweets.csv'\n",
    "    save_tweets_as_CSV(save_path, tweet_df)\n",
    "    \n",
    "    return tweet_df\n",
    "\n",
    "# Run scraper for each of \n",
    "# -----------------------\n",
    "# AI phrases : ['Automation', 'ArtificialIntelligence', 'AI'] With hashtag and without\n",
    "# plus\n",
    "# Psychology phrases : ['Automation', 'ArtificialIntelligence', 'AI'] With hashtag and without\n",
    "\n",
    "def get_phrases(phrase1, phrase2):\n",
    "    phrase_1_options = [symbol + phrase1 for symbol in ['#', '']]\n",
    "    phrase_2_options = [symbol + phrase2 for symbol in ['#', '']]\n",
    "    options = [phrase_1_options[0] + ' ' + phrase_2_options[0],\n",
    "               phrase_1_options[0] + ' ' + phrase_2_options[1],\n",
    "               phrase_1_options[1] + ' ' + phrase_2_options[0],\n",
    "               phrase_1_options[1] + ' ' + phrase_2_options[1],]\n",
    "    return options\n",
    "\n",
    "def get_all_phrases(phrases, phrase2):\n",
    "    search_phrases = [ get_phrases(phrase, phrase2) for phrase in phrases]\n",
    "    all_search_phrases = []\n",
    "    for phrases in zip(*search_phrases):\n",
    "        all_search_phrases.extend(phrases)\n",
    "    return all_search_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 48 phrases. Examples:\n",
      "-------------------------------\n",
      "-  #Automation #psychology\n",
      "-  #ArtificialIntelligence #psychology\n",
      "-  #AI #psychology\n",
      "-  #Automation psychology\n",
      "-  #ArtificialIntelligence psychology\n",
      "-  #AI psychology\n",
      "-  Automation #psychology\n",
      "-  ArtificialIntelligence #psychology\n",
      "-  AI #psychology\n",
      "-  Automation psychology\n",
      "\n",
      "+ 38 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example setting\n",
    "# ---------------\n",
    "# Search on 'phrase + ' ' psychology, both with and without hash prefix\n",
    "\n",
    "# Search phrases\n",
    "phrases = ['Automation', 'ArtificialIntelligence', 'AI']\n",
    "phrase2 = 'psychology' # human, sociology, markeing \n",
    "\n",
    "# Data was gathered in number of such runs with 'sociology', 'human', 'marketing in place of psychology'.\n",
    "\n",
    "\n",
    "# Save data to folder\n",
    "SAVE_DIR = 'tweets_data/'\n",
    "\n",
    "# Scraper auth & limits\n",
    "max_tweets_to_get = 50000\n",
    "max_attempts = 50\n",
    "twitter_auth_path = 'twitter_auth.json'\n",
    "\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Generate phrases\n",
    "all_search_phrases = get_all_phrases(phrases, phrase2)\n",
    "\n",
    "# Show example search phrases:\n",
    "show = 10\n",
    "num_files = len(all_search_phrases)*4\n",
    "print('\\nThere are {} phrases. Examples:\\n-------------------------------'.format(num_files))\n",
    "for phrase in all_search_phrases[:show]:\n",
    "    print(\"- \", phrase)\n",
    "print('\\n+ {} more\\n\\n'.format ((num_files - show)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search phrases: ['#Automation #human', '#ArtificialIntelligence #human', '#AI #human', '#Automation human', '#ArtificialIntelligence human', '#AI human', 'Automation #human', 'ArtificialIntelligence #human', 'AI #human', 'Automation human', 'ArtificialIntelligence human', 'AI human']\n",
      "\n",
      " 0 #Automation #human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 505.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "1    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "2    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "3    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "4    75% want more #human vs #machine interactions ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(505, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 tweets saved to tweets_data/#Automation #human_20190119202843_tweets.csv\n",
      "\n",
      " 1 #ArtificialIntelligence #human\n",
      "Loop: 1 finished. Tweets gathered sum: 99.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    RT @SpirosMargaris: Cognitive Computing: \\n\\nM...\n",
       "2    RT @SpirosMargaris: Cognitive Computing: \\n\\nM...\n",
       "3    RT @MikeQuindazzi: 1000's of simulated neurons...\n",
       "4    RT @MikeQuindazzi: 1000's of simulated neurons...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(99, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tweets saved to tweets_data/#ArtificialIntelligence #human_20190119202847_tweets.csv\n",
      "\n",
      " 2 #AI #human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 800.\n",
      "Loop: 9 finished. Tweets gathered sum: 900.\n",
      "Loop: 10 finished. Tweets gathered sum: 987.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "2    #Researchgate #Stats #UNINA #Naples #Universit...\n",
       "3    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "4    RT @vanguardsw: RT @chboursin \"#AI will achiev...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(987, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987 tweets saved to tweets_data/#AI #human_20190119202848_tweets.csv\n",
      "\n",
      " 3 #Automation human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 800.\n",
      "Loop: 9 finished. Tweets gathered sum: 900.\n",
      "Loop: 10 finished. Tweets gathered sum: 1000.\n",
      "Loop: 11 finished. Tweets gathered sum: 1100.\n",
      "Loop: 12 finished. Tweets gathered sum: 1122.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    How #AI Will Augment the Human Workforce: http...\n",
       "1    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "2    RT @KirkDBorne: How #AI Will Augment the Human...\n",
       "3    RT @ValinCorp: Check out Omron's TM Series Col...\n",
       "4    RT @ipfconline1: How Artificial Intelligence I...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1122, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122 tweets saved to tweets_data/#Automation human_20190119202855_tweets.csv\n",
      "\n",
      " 4 #ArtificialIntelligence human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 748.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    #ArtificialIntelligence has the great potentia...\n",
       "2    Is #AI about Taking Human Jobs or Creating The...\n",
       "3    RT @wil_bielert: RT @mvollmer1: When will the ...\n",
       "4    RT @FrRonconi: #Cognitive Computing: More Huma...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(748, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 tweets saved to tweets_data/#ArtificialIntelligence human_20190119202903_tweets.csv\n",
      "\n",
      " 5 #AI human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 800.\n",
      "Loop: 9 finished. Tweets gathered sum: 900.\n",
      "Loop: 10 finished. Tweets gathered sum: 1000.\n",
      "Loop: 11 finished. Tweets gathered sum: 1100.\n",
      "Loop: 12 finished. Tweets gathered sum: 1200.\n",
      "Loop: 13 finished. Tweets gathered sum: 1300.\n",
      "Loop: 14 finished. Tweets gathered sum: 1400.\n",
      "Loop: 15 finished. Tweets gathered sum: 1493.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    .@Nature article by @EricTopol on high-perform...\n",
       "1    How #AI Will Augment the Human Workforce: http...\n",
       "2    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "3    RT @wil_bielert: RT @mvollmer1: When will the ...\n",
       "4    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1493, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493 tweets saved to tweets_data/#AI human_20190119202908_tweets.csv\n",
      "\n",
      " 6 Automation #human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 513.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "1    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "2    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "3    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "4    75% want more #human vs #machine interactions ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(513, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 tweets saved to tweets_data/Automation #human_20190119202917_tweets.csv\n",
      "\n",
      " 7 ArtificialIntelligence #human\n",
      "Loop: 1 finished. Tweets gathered sum: 99.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    RT @SpirosMargaris: Cognitive Computing: \\n\\nM...\n",
       "2    RT @SpirosMargaris: Cognitive Computing: \\n\\nM...\n",
       "3    RT @MikeQuindazzi: 1000's of simulated neurons...\n",
       "4    RT @MikeQuindazzi: 1000's of simulated neurons...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(99, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tweets saved to tweets_data/ArtificialIntelligence #human_20190119202921_tweets.csv\n",
      "\n",
      " 8 AI #human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 800.\n",
      "Loop: 9 finished. Tweets gathered sum: 900.\n",
      "Loop: 10 finished. Tweets gathered sum: 1000.\n",
      "Loop: 11 finished. Tweets gathered sum: 1021.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "2    #Researchgate #Stats #UNINA #Naples #Universit...\n",
       "3    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "4    RT @vanguardsw: RT @chboursin \"#AI will achiev...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1021, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021 tweets saved to tweets_data/AI #human_20190119202922_tweets.csv\n",
      "\n",
      " 9 Automation human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 200.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    How #AI Will Augment the Human Workforce: http...\n",
       "1    From #AI to drones and #3Dprinting, emerging t...\n",
       "2    RT @BobSudothis: 75% want more #human vs #mach...\n",
       "3    RT @KirkDBorne: How #AI Will Augment the Human...\n",
       "4    RT @AiConstellation: In a world of increasing ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(200, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 tweets saved to tweets_data/Automation human_20190119202932_tweets.csv\n",
      "\n",
      " 10 ArtificialIntelligence human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 749.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\n\\nMore #Human Than #Art...\n",
       "1    Is #AI about Taking Human Jobs or Creating The...\n",
       "2    #ArtificialIntelligence has the great potentia...\n",
       "3    RT @mvollmer1: When will the #AI wave happen? ...\n",
       "4    RT @wil_bielert: RT @mvollmer1: When will the ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(749, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 tweets saved to tweets_data/ArtificialIntelligence human_20190119202934_tweets.csv\n",
      "\n",
      " 11 AI human\n",
      "Loop: 1 finished. Tweets gathered sum: 100.\n",
      "Loop: 2 finished. Tweets gathered sum: 200.\n",
      "Loop: 3 finished. Tweets gathered sum: 300.\n",
      "Loop: 4 finished. Tweets gathered sum: 400.\n",
      "Loop: 5 finished. Tweets gathered sum: 500.\n",
      "Loop: 6 finished. Tweets gathered sum: 600.\n",
      "Loop: 7 finished. Tweets gathered sum: 700.\n",
      "Loop: 8 finished. Tweets gathered sum: 800.\n",
      "Loop: 9 finished. Tweets gathered sum: 900.\n",
      "Loop: 10 finished. Tweets gathered sum: 1000.\n",
      "Loop: 11 finished. Tweets gathered sum: 1100.\n",
      "Loop: 12 finished. Tweets gathered sum: 1200.\n",
      "Loop: 13 finished. Tweets gathered sum: 1300.\n",
      "Loop: 14 finished. Tweets gathered sum: 1400.\n",
      "Loop: 15 finished. Tweets gathered sum: 1500.\n",
      "Loop: 16 finished. Tweets gathered sum: 1600.\n",
      "Loop: 17 finished. Tweets gathered sum: 1700.\n",
      "Loop: 18 finished. Tweets gathered sum: 1800.\n",
      "Loop: 19 finished. Tweets gathered sum: 1900.\n",
      "Loop: 20 finished. Tweets gathered sum: 2000.\n",
      "Loop: 21 finished. Tweets gathered sum: 2100.\n",
      "Loop: 22 finished. Tweets gathered sum: 2200.\n",
      "Loop: 23 finished. Tweets gathered sum: 2300.\n",
      "Loop: 24 finished. Tweets gathered sum: 2400.\n",
      "Loop: 25 finished. Tweets gathered sum: 2500.\n",
      "Loop: 26 finished. Tweets gathered sum: 2600.\n",
      "Loop: 27 finished. Tweets gathered sum: 2700.\n",
      "Loop: 28 finished. Tweets gathered sum: 2800.\n",
      "Loop: 29 finished. Tweets gathered sum: 2900.\n",
      "Loop: 30 finished. Tweets gathered sum: 3000.\n",
      "Loop: 31 finished. Tweets gathered sum: 3100.\n",
      "Loop: 32 finished. Tweets gathered sum: 3200.\n",
      "Loop: 33 finished. Tweets gathered sum: 3300.\n",
      "Loop: 34 finished. Tweets gathered sum: 3400.\n",
      "Loop: 35 finished. Tweets gathered sum: 3500.\n",
      "Loop: 36 finished. Tweets gathered sum: 3600.\n",
      "Loop: 37 finished. Tweets gathered sum: 3700.\n",
      "Loop: 38 finished. Tweets gathered sum: 3800.\n",
      "Loop: 39 finished. Tweets gathered sum: 3900.\n",
      "Loop: 40 finished. Tweets gathered sum: 4000.\n",
      "Loop: 41 finished. Tweets gathered sum: 4099.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    There’s no way that a malevolent AI could ever...\n",
       "1    How #AI Will Augment the Human Workforce: http...\n",
       "2    Pair of ancient skeletons thought to represent...\n",
       "3    RT @MiaD: Super excited to kick off our first ...\n",
       "4    RT @mvollmer1: When will the #AI wave happen? ...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4099, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4099 tweets saved to tweets_data/AI human_20190119202939_tweets.csv\n",
      "12 dataframes collected with total 11635 of samples.\n"
     ]
    }
   ],
   "source": [
    "# Run scraper \n",
    "all_searches_data = []\n",
    "for i, search_phrase in enumerate(all_search_phrases):\n",
    "    print('\\n', i, search_phrase)\n",
    "    search_phrase_tweets_df = get_tweets_by_search_phrase(search_phrase, max_tweets_to_get, max_attempts, \n",
    "                                                          twitter_auth_path, SAVE_DIR)\n",
    "    all_searches_data.append(search_phrase_tweets_df)\n",
    "\n",
    "num_of_frames = len(all_searches_data)\n",
    "num_of_rows = sum([df.shape[0] for df in all_searches_data])\n",
    "print('{} dataframes collected with total {} of samples.'.format(num_of_frames, num_of_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSVs size: (24416, 23)\n",
      "\n",
      "All CSVs cols: Index(['index', 'account_created_at', 'author', 'author_description',\n",
      "       'author_favourites_count', 'author_followers_count',\n",
      "       'author_friends_count', 'author_id', 'author_listed_count',\n",
      "       'author_location', 'author_statuses_count', 'created_at',\n",
      "       'favorite_count', 'file_tag', 'hashtags', 'in_reply_to_screen_name',\n",
      "       'in_reply_to_status_id', 'lang', 'retweet_count', 'scrap_phrase',\n",
      "       'scrap_time', 'text', 'tweet_id'],\n",
      "      dtype='object')\n",
      "\n",
      "Check tweet language distribution:\n",
      "en                24394\n",
      "AI human              2\n",
      "#AI human             2\n",
      "#AI psychology        1\n",
      "AI psychology         1\n",
      "Name: lang, dtype: int64\n",
      "\n",
      "\n",
      "Check for duplicates\n",
      "- Nr of entries 24394\n",
      "- Uniq  entries 6817\n",
      "\n",
      "\n",
      "Distribution of unique, english tweets over category tag [phrase, hash]:\n",
      "AI human                                 1516\n",
      "#AI marketing                            1138\n",
      "#AI #marketing                            749\n",
      "#AI human                                 600\n",
      "#ArtificialIntelligence #marketing        462\n",
      "#ArtificialIntelligence human             333\n",
      "Automation marketing                      309\n",
      "#Automation human                         279\n",
      "#AI #human                                268\n",
      "#ArtificialIntelligence marketing         180\n",
      "#Automation #marketing                    149\n",
      "Automation #marketing                     120\n",
      "AI #marketing                             110\n",
      "AI psychology                             110\n",
      "#AI psychology                             93\n",
      "Automation human                           91\n",
      "#Automation marketing                      47\n",
      "#AI #psychology                            44\n",
      "AI #neuroscience                           35\n",
      "#ArtificialIntelligence #neuroscience      29\n",
      "#AI #neuroscience                          28\n",
      "#ArtificialIntelligence #human             24\n",
      "#ArtificialIntelligence psychology         22\n",
      "#Automation #human                         21\n",
      "AI #human                                  21\n",
      "#ArtificialIntelligence neuroscience        6\n",
      "AI neuroscience                             6\n",
      "Automation psychology                       6\n",
      "#ArtificialIntelligence #psychology         5\n",
      "Automation #human                           4\n",
      "AI #psychology                              3\n",
      "AI #sociology                               3\n",
      "#AI neuroscience                            2\n",
      "ArtificialIntelligence human                2\n",
      "ArtificialIntelligence #marketing           1\n",
      "#Automation #psychology                     1\n",
      "Name: scrap_phrase, dtype: int64\n",
      "6817 tweets saved to AI_Psycho_tweets.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Combined_dataset of shape (6817, 23):'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....</td>\n",
       "      <td>Human ArtificialIntelligence fintech insurtech AI MachineLearning DeepLearning robotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...</td>\n",
       "      <td>human machine CX futureofwork PwC AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...</td>\n",
       "      <td>Researchgate Stats UNINA Naples University Researchers VanGELOAssoluto Federico2 book GOD Human ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @vanguardsw: RT @chboursin \"#AI will achieve #human-like #skills &amp;gt; 2026: write high-school...</td>\n",
       "      <td>AI human skills</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....   \n",
       "1  RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...   \n",
       "2  #Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...   \n",
       "3  RT @vanguardsw: RT @chboursin \"#AI will achieve #human-like #skills &gt; 2026: write high-school...   \n",
       "\n",
       "                                                                                              hashtags  \n",
       "0             Human ArtificialIntelligence fintech insurtech AI MachineLearning DeepLearning robotics   \n",
       "1                                                                human machine CX futureofwork PwC AI   \n",
       "2  Researchgate Stats UNINA Naples University Researchers VanGELOAssoluto Federico2 book GOD Human ...  \n",
       "3                                                                                     AI human skills   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "# 1. Make one 24.4k samples DataFrame out of ~50 CSV's\n",
    "\n",
    "def get_dsets_together(folder_path,sep='\\t',encoding='utf-8'):\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    num_files = len(all_files)\n",
    "\n",
    "    dfs = []\n",
    "    for i, file in enumerate(all_files):\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, encoding=encoding)\n",
    "            if df.shape[0] > 0:\n",
    "                dfs.append(df)\n",
    "        except:\n",
    "            print('--not-loaded-->', file)       \n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "folder_path = r'C:\\\\Users\\\\p\\\\Projects\\\\100analysis\\\\tweets_data' \n",
    "df =get_dsets_together(folder_path)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "print('All CSVs size:', df.shape)\n",
    "print('\\nAll CSVs cols:', df.columns)\n",
    "\n",
    "# 1. Check tweets language, remove non english.\n",
    "print('\\nCheck tweet language distribution:\\n{}'.format(df.lang.value_counts().head(10)))\n",
    "df = df[(df.lang == 'en')]\n",
    "\n",
    "# 2. Check, remove duplicates and reset index.\n",
    "print('\\n\\nCheck for duplicates\\n- Nr of entries {}\\n- Uniq  entries {}'.format(df.shape[0], \n",
    "                                                                                df.text.unique().shape[0]))\n",
    "df = df.drop_duplicates(subset='text', keep='first')\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 3. Check distribution of tweets over categories\n",
    "print('\\n\\nDistribution of unique, english tweets over category tag [phrase, hash]:\\n{}'.format(df.scrap_phrase.value_counts()))\n",
    "\n",
    "# 4. Save & reload, backup\n",
    "save_tweets_as_CSV('AI_Psycho_tweets.csv', df)\n",
    "df = pd.read_csv('AI_Psycho_tweets.csv', sep='\\t', encoding='utf-8')\n",
    "backup = df.copy(deep=True)\n",
    "\n",
    "# 5. Check shape and look\n",
    "display('Combined_dataset of shape {}:'.format(df.shape), df.loc[:,['text','hashtags']].head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6817, 24)\n",
      "Index(['index', 'account_created_at', 'author', 'author_description',\n",
      "       'author_favourites_count', 'author_followers_count',\n",
      "       'author_friends_count', 'author_id', 'author_listed_count',\n",
      "       'author_location', 'author_statuses_count', 'created_at',\n",
      "       'favorite_count', 'file_tag', 'hashtags', 'in_reply_to_screen_name',\n",
      "       'in_reply_to_status_id', 'lang', 'retweet_count', 'scrap_phrase',\n",
      "       'scrap_time', 'text', 'tweet_id', 'text_raw'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retweet_count</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>favorite_count</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Column name    dtype\n",
       "0            text   object\n",
       "1   retweet_count  float64\n",
       "2  favorite_count  float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @vanguardsw: RT @chboursin \"#AI will achieve #human-like #skills &amp;gt; 2026: write high-school...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75% want more #human vs #machine interactions to improve #CX in the #futureofwork &amp;gt;&amp;gt;&amp;gt; #...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....   \n",
       "1  RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...   \n",
       "2  #Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...   \n",
       "3  RT @vanguardsw: RT @chboursin \"#AI will achieve #human-like #skills &gt; 2026: write high-school...   \n",
       "4  75% want more #human vs #machine interactions to improve #CX in the #futureofwork &gt;&gt;&gt; #...   \n",
       "\n",
       "   retweet_count  favorite_count  \n",
       "0           26.0            32.0  \n",
       "1            4.0             0.0  \n",
       "2            0.0             0.0  \n",
       "3            7.0             0.0  \n",
       "4            4.0             2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>Nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_description</td>\n",
       "      <td>{&lt;class 'float'&gt;, &lt;class 'str'&gt;}</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_location</td>\n",
       "      <td>{&lt;class 'float'&gt;, &lt;class 'str'&gt;}</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in_reply_to_screen_name</td>\n",
       "      <td>{&lt;class 'str'&gt;, &lt;class 'float'&gt;}</td>\n",
       "      <td>6483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in_reply_to_status_id</td>\n",
       "      <td></td>\n",
       "      <td>6517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet_id</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    column                            dtypes   Nan\n",
       "0       author_description  {<class 'float'>, <class 'str'>}   422\n",
       "1          author_location  {<class 'float'>, <class 'str'>}  1148\n",
       "2  in_reply_to_screen_name  {<class 'str'>, <class 'float'>}  6483\n",
       "3    in_reply_to_status_id                                    6517\n",
       "4                 tweet_id                                       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore data quality: types, nans, strange values\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Check df for Nans and suspicious dtypes\n",
    "def get_suspicious_columns_data(df):\n",
    "    suspicious =[]\n",
    "    for column in df.columns:\n",
    "    \n",
    "        raport_data = [column]\n",
    "    \n",
    "        # Nan values sum\n",
    "        nan_sum = df[column].isna().sum()\n",
    "        \n",
    "        # Undefined datatypes\n",
    "        c_types = set([type(val) for val in df[column].tolist()])\n",
    "    \n",
    "    \n",
    "        if len(c_types) > 1 and nan_sum > 0:\n",
    "            raport_data.append(str(c_types))\n",
    "            raport_data.append(nan_sum)\n",
    "        elif len(c_types) > 1 and not nan_sum > 0:\n",
    "            raport_data.append(str(c_types))\n",
    "            raport_data.append(0)\n",
    "        elif not len(c_types) > 1 and nan_sum > 0:\n",
    "            raport_data.append('')\n",
    "            raport_data.append(nan_sum)\n",
    "        \n",
    "        if len(raport_data) > 1:\n",
    "            suspicious.append(raport_data)\n",
    "    return suspicious\n",
    "    \n",
    "\n",
    "# Explore\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "see = ['text', 'retweet_count', 'favorite_count']\n",
    "data_types = pd.DataFrame([[col, df[col].dtype] for col in see],columns=['Column name','dtype'])\n",
    "\n",
    "display(data_types)\n",
    "display(df.loc[:, see].head(5))\n",
    "display(pd.DataFrame(get_suspicious_columns_data(df), columns=['column','dtypes','Nan']))\n",
    "# This are ok.\n",
    "\n",
    "# Check values at retweet_count, favorite_count, scrap_phrase etc.\n",
    "#df.scrap_phrase.value_counts().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....\n",
       "1    RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...\n",
       "2    #Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...\n",
       "3    RT @vanguardsw: RT @chboursin \"#AI will achieve #human-like #skills &gt; 2026: write high-school...\n",
       "4    75% want more #human vs #machine interactions to improve #CX in the #futureofwork &gt;&gt;&gt; #...\n",
       "5    RT @SpirosMargaris: Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r...\n",
       "6    RT @Windy07041: This #interview really is #groundbreaking. I can't wrap my #head all the way aro...\n",
       "7    RT @TPerplexa: More about the #Qanon operation\\r\\r\\nLearn about #AI, #SOCI, basic #Human psychol...\n",
       "8    75% want more #human vs #machine interactions to improve #CX in the #futureofwork \\r\\r\\n\\r\\r\\n#A...\n",
       "9    RT @AngelHealthTech: Computing power + #AI #Algorithms on the path to exceed #human intelligence...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RETRIVE:\n",
    "# -------\n",
    "\n",
    "df = pd.read_csv('AI_Psycho_tweets.csv', sep='\\t', encoding='utf-8')\n",
    "del df['index'] #lol\n",
    "backup = df.copy(deep=True)\n",
    "df.text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text for exploration\n",
    "# ----------------------------\n",
    "\n",
    "# 1. URL and text without\n",
    "def get_url(text):\n",
    "    try: \n",
    "        return re.search(\"(?P<url>https?://[^\\s]+)\", text).group(\"url\")\n",
    "    except: \n",
    "        return ''\n",
    "def remove_url(text):\n",
    "    return re.sub('https?://[A-Za-z0-9./]+','',text)\n",
    "def get_and_remove_url_from_text(text):\n",
    "    return get_url(text), remove_url(text)\n",
    "\n",
    "# 2. RT sign and text without it\n",
    "def get_and_remove_rt_from_text(text):\n",
    "    if text[:2] == 'RT':\n",
    "        return 1, text[3:]\n",
    "    else:\n",
    "        return 0, text\n",
    "\n",
    "# 3. Hashtags into words, mentions into 'PERSON'\n",
    "def get_mentions(text):\n",
    "    return ' '.join([w[1:] for w in text.split(' ') if w.startswith('@')])\n",
    "def get_hashtags(text):\n",
    "    return ' '.join([w[1:] for w in text.split(' ') if w.startswith('#')])\n",
    "\n",
    "# 3b. Hash, mention strategies\n",
    "def hashtags_and_mentions_to_text(text):\n",
    "    string = \"\"\n",
    "    for w in text.split():\n",
    "        if w.startswith('@') or w.startswith('#'):\n",
    "            string = \"\".join([string, ' ', w[1:]])\n",
    "        else:\n",
    "            string = \"\".join([string, ' ', w])            \n",
    "    return string\n",
    "def get_raw_holders(text):\n",
    "    text = text.split()\n",
    "    raw_holders = ''\n",
    "    for word in text:\n",
    "        if word.startswith('@'):\n",
    "            raw_holders += ' PERSON'\n",
    "        elif word.startswith('#'):\n",
    "            raw_holders += ' HASH'\n",
    "        else:\n",
    "            raw_holders += ' WORD'\n",
    "    return raw_holders[1:]\n",
    "def get_placeholders(text):\n",
    "    text = text.split()\n",
    "    placeholders = ''\n",
    "    for word in text:\n",
    "        if word.startswith('@'):\n",
    "            placeholders += ' PERSON'\n",
    "        elif word.startswith('#'):\n",
    "            placeholders += ' HASH'\n",
    "        else:\n",
    "            placeholders += ' ' + word\n",
    "    return placeholders[1:]\n",
    "def get_hash_indexes(text):\n",
    "    hash_idxs, at_idxs, w_idxs = '', '', ''\n",
    "    for i, w in enumerate(text.split()):\n",
    "        if w.startswith('#'):\n",
    "            hash_idxs += ' ' + str(i)\n",
    "        elif w.startswith('@'):\n",
    "            at_idxs += ' ' + str(i)\n",
    "        else:\n",
    "            w_idxs  += ' ' + str(i)\n",
    "    return hash_idxs[1:], at_idxs[1:], w_idxs[1:]\n",
    "\n",
    "def get_hash_indexes_DEHASHED(text):\n",
    "    hash_idxs, at_idxs, w_idxs = '', '', ''\n",
    "    for i, w in enumerate(text.split()):\n",
    "        if w == 'HASH':\n",
    "            hash_idxs += ' ' + str(i)\n",
    "        elif w == 'PERSON':\n",
    "            at_idxs += ' ' + str(i)\n",
    "        else:\n",
    "            w_idxs  += ' ' + str(i)\n",
    "    return hash_idxs[1:], at_idxs[1:], w_idxs[1:]\n",
    "\n",
    "# ----> ADD GETTING WORDS OUT OF HASHTAG/MENTION CAMMELCASE\n",
    "\n",
    "# 4. Date as a feature\n",
    "def get_date_day(date):\n",
    "    return int(date.split(' ')[2])\n",
    "\n",
    "# 5. Clean the text\n",
    "def strip_html(text, praser='lxml'): # lxml', 'html.parser'\n",
    "    return BeautifulSoup(text, praser).get_text()\n",
    "def BOM_replace(text):\n",
    "    try:\n",
    "        return text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        return text\n",
    "def strip_inner_spaces(text):\n",
    "    return ' '.join([w.strip() for w in text.split()])\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# 6. Prepare text for analysis: lemmatize, remove stop words & special characters\n",
    "def lemmatize_words(text):\n",
    "    WNL = WordNetLemmatizer()\n",
    "    return ' '.join([WNL.lemmatize(word, pos='v') for word in text.split()])\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([w for w in text.split() if not w in set(stopwords.words('english'))])\n",
    "\n",
    "# 7. Get all Tag words in texts at one (4-6 times faster)\n",
    "def get_text_tokens(text):\n",
    "    # Token-type containers for text\n",
    "    poses, tags, deps, heads, idxes = [], [], [], [], []\n",
    "    # Get tokens\n",
    "    for i, token in enumerate(nlp(text)):\n",
    "        if token.is_space == False:\n",
    "            deps.append(token.dep_)\n",
    "            heads.append(token.head.text)\n",
    "            tags.append(token.tag_)\n",
    "            poses.append(token.pos_)\n",
    "            idxes.append(token.idx)\n",
    "        \n",
    "    # Placeholders, each list as a string\n",
    "    results = []\n",
    "    for tl in [poses, tags, deps, heads, idxes]:\n",
    "        if len(tl) == 0:\n",
    "            tl = ['']*i\n",
    "        if not tl[0] is str:\n",
    "            tl = [str(a) for a in tl]\n",
    "        result = ' '.join(tl)\n",
    "        results.append(result)\n",
    "    # Word index (word order)\n",
    "    results.append(' '.join([str(nr) for nr in range(i)])) \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features, hashtag, mentions done: 0:00:01.026058 0:00:01.026058\n",
      "Text cleaning pipe done: 0:00:06.871393 0:00:05.845335\n",
      "Hashes strategy, prep, sanity, reindex: 0:02:12.896474 0:02:06.025081\n",
      "Spacy tags: 0:12:49.859906 0:10:36.963432\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>account_created_at</th>\n",
       "      <td>Wed Aug 06 05:20:16 +0000 2014</td>\n",
       "      <td>Thu Mar 15 04:51:22 +0000 2018</td>\n",
       "      <td>Sun Jun 10 18:04:31 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>SpirosMargaris</td>\n",
       "      <td>virginiakelly78</td>\n",
       "      <td>SalViVicidomini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_description</th>\n",
       "      <td>@wefoxHQ @SparkLabsGlobal @GetHufsy @LodexAus @MediaStalker1 @ArbidexToken @F10_accelerator | No...</td>\n",
       "      <td>Researcher, Compiler, Professional Troublemaker. :)</td>\n",
       "      <td>♈Sono nato, e prima o poi morrò! Il resto è rumore entropico! #FB https://t.co/FmPvrmCh0w ●●My #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_favourites_count</th>\n",
       "      <td>118656</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_followers_count</th>\n",
       "      <td>68699</td>\n",
       "      <td>36</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_friends_count</th>\n",
       "      <td>10429</td>\n",
       "      <td>42</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_id</th>\n",
       "      <td>2.71121e+09</td>\n",
       "      <td>9.74146e+17</td>\n",
       "      <td>6.04715e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_listed_count</th>\n",
       "      <td>5260</td>\n",
       "      <td>2</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_location</th>\n",
       "      <td>All Over the World</td>\n",
       "      <td>Kansas City, MO</td>\n",
       "      <td>EARTH.pk, 2.O !!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_statuses_count</th>\n",
       "      <td>135407</td>\n",
       "      <td>455</td>\n",
       "      <td>17289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <td>Fri Jan 18 06:52:01 +0000 2019</td>\n",
       "      <td>Sat Jan 19 19:25:38 +0000 2019</td>\n",
       "      <td>Sat Jan 19 18:50:06 +0000 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite_count</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_tag</th>\n",
       "      <td>2019-01-19T20:28:48</td>\n",
       "      <td>2019-01-19T20:28:48</td>\n",
       "      <td>2019-01-19T20:28:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtags</th>\n",
       "      <td>Human ArtificialIntelligence fintech insurtech AI MachineLearning DeepLearning robotics</td>\n",
       "      <td>human machine CX futureofwork PwC AI Aut…</td>\n",
       "      <td>Researchgate Stats UNINA Naples University Researchers VanGELOAssoluto Federico2 book GOD Human ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweet_count</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrap_phrase</th>\n",
       "      <td>#AI #human</td>\n",
       "      <td>#AI #human</td>\n",
       "      <td>#AI #human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrap_time</th>\n",
       "      <td>2019-01-19T20:28:48.670012</td>\n",
       "      <td>2019-01-19T20:28:48.670012</td>\n",
       "      <td>2019-01-19T20:28:48.670012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>cognitive compute human artificialintelligence intengineering thisdotjohn fintech insurtech ai m...</td>\n",
       "      <td>bobsudothis want human vs machine interactions improve cx futureofwork pwc via mikequindazzi ai aut</td>\n",
       "      <td>researchgate stats unina naples university researchers vangeloassoluto federico book god human e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <td>1.08615e+18</td>\n",
       "      <td>1.08671e+18</td>\n",
       "      <td>1.0867e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_raw</th>\n",
       "      <td>Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....</td>\n",
       "      <td>RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...</td>\n",
       "      <td>#Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>link</th>\n",
       "      <td>https://t.co/EmekqoYmub</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/UlxbL26VuL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_hash</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentions</th>\n",
       "      <td>IntEngineering ThisDotJohn Ronald_vanLoon pierrepinna Paula_Piccard YuHelenYu</td>\n",
       "      <td>BobSudothis: MikeQuindazzi</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_mentions</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dehashed</th>\n",
       "      <td>cognitive computing: more human than artificialintelligence intengineering thisdotjohn fintech ...</td>\n",
       "      <td>bobsudothis: 75% want more human vs machine interactions to improve cx in the futureofwork &gt;&gt;&gt; ...</td>\n",
       "      <td>researchgate stats unina naples university researchers vangeloassoluto federico2 book of god &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_holders</th>\n",
       "      <td>WORD WORD WORD HASH WORD HASH PERSON PERSON HASH HASH HASH HASH HASH HASH PERSON PERSON PERSON P...</td>\n",
       "      <td>PERSON WORD WORD WORD HASH WORD HASH WORD WORD WORD HASH WORD WORD HASH WORD HASH WORD PERSON WO...</td>\n",
       "      <td>HASH HASH HASH HASH HASH HASH HASH HASH HASH WORD HASH WORD HASH HASH WORD HASH HASH HASH HASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placeholders</th>\n",
       "      <td>cognitive computing: more HASH than HASH PERSON PERSON HASH HASH HASH HASH HASH HASH PERSON PERS...</td>\n",
       "      <td>PERSON 75% want more HASH vs HASH interactions to improve HASH in the HASH &gt;&gt;&gt; HASH via PERSON &gt;...</td>\n",
       "      <td>HASH HASH HASH HASH HASH HASH HASH HASH HASH of HASH &amp; HASH HASH (via HASH HASH HASH HASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_h_idx</th>\n",
       "      <td>3 5 8 9 10 11 12 13</td>\n",
       "      <td>4 6 10 13 15 19 20</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 10 12 13 15 16 17 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_at_idx</th>\n",
       "      <td>6 7 14 15 16 17</td>\n",
       "      <td>0 17</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_w_idx</th>\n",
       "      <td>0 1 2 4</td>\n",
       "      <td>1 2 3 5 7 8 9 11 12 14 16 18</td>\n",
       "      <td>9 11 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANITY 1 w</th>\n",
       "      <td>6817</td>\n",
       "      <td>6817</td>\n",
       "      <td>6817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANITY 1 c</th>\n",
       "      <td>210</td>\n",
       "      <td>137</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANITY 2 w</th>\n",
       "      <td>6817</td>\n",
       "      <td>6817</td>\n",
       "      <td>6817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SANITY 2 c</th>\n",
       "      <td>183</td>\n",
       "      <td>99</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POSES</th>\n",
       "      <td>ADJ NOUN ADJ NOUN VERB NOUN NOUN ADJ NOUN VERB VERB NOUN ADP NOUN NOUN NOUN ADJ NOUN</td>\n",
       "      <td>INTJ VERB ADJ ADP NOUN NOUN VERB NOUN NOUN NOUN ADP NOUN VERB NOUN</td>\n",
       "      <td>NOUN NOUN ADJ VERB NOUN NOUN VERB NOUN NOUN NOUN ADJ NOUN ADP NOUN NOUN NOUN NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAGS</th>\n",
       "      <td>JJ NN JJ NN VBG NN NN JJ NN VBG VBG NNS IN NN NN NN JJ NN</td>\n",
       "      <td>UH VBP JJ IN NN NNS VBP NN NN NN IN NN VBP NN</td>\n",
       "      <td>NN NNS JJ VBZ NN NNS VBP NN NN NN JJ NN IN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPS</th>\n",
       "      <td>amod nmod amod nsubj ROOT compound compound nsubj aux ccomp amod dobj dep amod compound compound...</td>\n",
       "      <td>nsubj csubj amod nmod compound nsubj ccomp compound nsubj dobj prep pobj ROOT attr</td>\n",
       "      <td>amod nsubj compound compound compound nsubj ROOT compound dobj intj amod appos prep punct compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEADS</th>\n",
       "      <td>compute artificialintelligence artificialintelligence intengineering intengineering fintech insu...</td>\n",
       "      <td>want ai interactions interactions interactions improve want futureofwork pwc improve improve via...</td>\n",
       "      <td>stats vangeloassoluto naples university researchers vangeloassoluto vangeloassoluto book vangelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDXS</th>\n",
       "      <td>0 10 18 24 47 62 74 82 92 95 111 124 133 140 148 160 166 174</td>\n",
       "      <td>0 12 17 23 26 34 47 55 58 71 75 79 93 96</td>\n",
       "      <td>0 13 19 25 32 43 55 71 80 85 89 95 105 109 112 118 126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W_IDXS</th>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           0  \\\n",
       "account_created_at                                                                            Wed Aug 06 05:20:16 +0000 2014   \n",
       "author                                                                                                        SpirosMargaris   \n",
       "author_description       @wefoxHQ @SparkLabsGlobal @GetHufsy @LodexAus @MediaStalker1 @ArbidexToken @F10_accelerator | No...   \n",
       "author_favourites_count                                                                                               118656   \n",
       "author_followers_count                                                                                                 68699   \n",
       "author_friends_count                                                                                                   10429   \n",
       "author_id                                                                                                        2.71121e+09   \n",
       "author_listed_count                                                                                                     5260   \n",
       "author_location                                                                                           All Over the World   \n",
       "author_statuses_count                                                                                                 135407   \n",
       "created_at                                                                                    Fri Jan 18 06:52:01 +0000 2019   \n",
       "favorite_count                                                                                                            32   \n",
       "file_tag                                                                                                 2019-01-19T20:28:48   \n",
       "hashtags                             Human ArtificialIntelligence fintech insurtech AI MachineLearning DeepLearning robotics   \n",
       "in_reply_to_screen_name                                                                                                  NaN   \n",
       "in_reply_to_status_id                                                                                                    NaN   \n",
       "lang                                                                                                                      en   \n",
       "retweet_count                                                                                                             26   \n",
       "scrap_phrase                                                                                                      #AI #human   \n",
       "scrap_time                                                                                        2019-01-19T20:28:48.670012   \n",
       "text                     cognitive compute human artificialintelligence intengineering thisdotjohn fintech insurtech ai m...   \n",
       "tweet_id                                                                                                         1.08615e+18   \n",
       "text_raw                 Cognitive Computing: \\r\\r\\n\\r\\r\\nMore #Human Than #ArtificialIntelligence \\r\\r\\n\\r\\r\\nhttps://t....   \n",
       "link                                                                                                 https://t.co/EmekqoYmub   \n",
       "RT                                                                                                                         0   \n",
       "day                                                                                                                       18   \n",
       "n_hash                                                                                                                     8   \n",
       "mentions                                       IntEngineering ThisDotJohn Ronald_vanLoon pierrepinna Paula_Piccard YuHelenYu   \n",
       "n_mentions                                                                                                                 6   \n",
       "dehashed                  cognitive computing: more human than artificialintelligence intengineering thisdotjohn fintech ...   \n",
       "raw_holders              WORD WORD WORD HASH WORD HASH PERSON PERSON HASH HASH HASH HASH HASH HASH PERSON PERSON PERSON P...   \n",
       "placeholders             cognitive computing: more HASH than HASH PERSON PERSON HASH HASH HASH HASH HASH HASH PERSON PERS...   \n",
       "raw_h_idx                                                                                                3 5 8 9 10 11 12 13   \n",
       "raw_at_idx                                                                                                   6 7 14 15 16 17   \n",
       "raw_w_idx                                                                                                            0 1 2 4   \n",
       "SANITY 1 w                                                                                                              6817   \n",
       "SANITY 1 c                                                                                                               210   \n",
       "SANITY 2 w                                                                                                              6817   \n",
       "SANITY 2 c                                                                                                               183   \n",
       "POSES                                   ADJ NOUN ADJ NOUN VERB NOUN NOUN ADJ NOUN VERB VERB NOUN ADP NOUN NOUN NOUN ADJ NOUN   \n",
       "TAGS                                                               JJ NN JJ NN VBG NN NN JJ NN VBG VBG NNS IN NN NN NN JJ NN   \n",
       "DEPS                     amod nmod amod nsubj ROOT compound compound nsubj aux ccomp amod dobj dep amod compound compound...   \n",
       "HEADS                    compute artificialintelligence artificialintelligence intengineering intengineering fintech insu...   \n",
       "IDXS                                                            0 10 18 24 47 62 74 82 92 95 111 124 133 140 148 160 166 174   \n",
       "W_IDXS                                                                              0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16   \n",
       "\n",
       "                                                                                                                           1  \\\n",
       "account_created_at                                                                            Thu Mar 15 04:51:22 +0000 2018   \n",
       "author                                                                                                       virginiakelly78   \n",
       "author_description                                                       Researcher, Compiler, Professional Troublemaker. :)   \n",
       "author_favourites_count                                                                                                    1   \n",
       "author_followers_count                                                                                                    36   \n",
       "author_friends_count                                                                                                      42   \n",
       "author_id                                                                                                        9.74146e+17   \n",
       "author_listed_count                                                                                                        2   \n",
       "author_location                                                                                              Kansas City, MO   \n",
       "author_statuses_count                                                                                                    455   \n",
       "created_at                                                                                    Sat Jan 19 19:25:38 +0000 2019   \n",
       "favorite_count                                                                                                             0   \n",
       "file_tag                                                                                                 2019-01-19T20:28:48   \n",
       "hashtags                                                                           human machine CX futureofwork PwC AI Aut…   \n",
       "in_reply_to_screen_name                                                                                                  NaN   \n",
       "in_reply_to_status_id                                                                                                    NaN   \n",
       "lang                                                                                                                      en   \n",
       "retweet_count                                                                                                              4   \n",
       "scrap_phrase                                                                                                      #AI #human   \n",
       "scrap_time                                                                                        2019-01-19T20:28:48.670012   \n",
       "text                     bobsudothis want human vs machine interactions improve cx futureofwork pwc via mikequindazzi ai aut   \n",
       "tweet_id                                                                                                         1.08671e+18   \n",
       "text_raw                 RT @BobSudothis: 75% want more #human vs #machine interactions to improve #CX in the #futureofwo...   \n",
       "link                                                                                                                           \n",
       "RT                                                                                                                         1   \n",
       "day                                                                                                                       19   \n",
       "n_hash                                                                                                                     7   \n",
       "mentions                                                                                          BobSudothis: MikeQuindazzi   \n",
       "n_mentions                                                                                                                 2   \n",
       "dehashed                  bobsudothis: 75% want more human vs machine interactions to improve cx in the futureofwork >>> ...   \n",
       "raw_holders              PERSON WORD WORD WORD HASH WORD HASH WORD WORD WORD HASH WORD WORD HASH WORD HASH WORD PERSON WO...   \n",
       "placeholders             PERSON 75% want more HASH vs HASH interactions to improve HASH in the HASH >>> HASH via PERSON >...   \n",
       "raw_h_idx                                                                                                 4 6 10 13 15 19 20   \n",
       "raw_at_idx                                                                                                              0 17   \n",
       "raw_w_idx                                                                                       1 2 3 5 7 8 9 11 12 14 16 18   \n",
       "SANITY 1 w                                                                                                              6817   \n",
       "SANITY 1 c                                                                                                               137   \n",
       "SANITY 2 w                                                                                                              6817   \n",
       "SANITY 2 c                                                                                                                99   \n",
       "POSES                                                     INTJ VERB ADJ ADP NOUN NOUN VERB NOUN NOUN NOUN ADP NOUN VERB NOUN   \n",
       "TAGS                                                                           UH VBP JJ IN NN NNS VBP NN NN NN IN NN VBP NN   \n",
       "DEPS                                      nsubj csubj amod nmod compound nsubj ccomp compound nsubj dobj prep pobj ROOT attr   \n",
       "HEADS                    want ai interactions interactions interactions improve want futureofwork pwc improve improve via...   \n",
       "IDXS                                                                                0 12 17 23 26 34 47 55 58 71 75 79 93 96   \n",
       "W_IDXS                                                                                          0 1 2 3 4 5 6 7 8 9 10 11 12   \n",
       "\n",
       "                                                                                                                           2  \n",
       "account_created_at                                                                            Sun Jun 10 18:04:31 +0000 2012  \n",
       "author                                                                                                       SalViVicidomini  \n",
       "author_description       ♈Sono nato, e prima o poi morrò! Il resto è rumore entropico! #FB https://t.co/FmPvrmCh0w ●●My #...  \n",
       "author_favourites_count                                                                                                   62  \n",
       "author_followers_count                                                                                                   181  \n",
       "author_friends_count                                                                                                     707  \n",
       "author_id                                                                                                        6.04715e+08  \n",
       "author_listed_count                                                                                                      141  \n",
       "author_location                                                                                            EARTH.pk, 2.O !!!  \n",
       "author_statuses_count                                                                                                  17289  \n",
       "created_at                                                                                    Sat Jan 19 18:50:06 +0000 2019  \n",
       "favorite_count                                                                                                             0  \n",
       "file_tag                                                                                                 2019-01-19T20:28:48  \n",
       "hashtags                 Researchgate Stats UNINA Naples University Researchers VanGELOAssoluto Federico2 book GOD Human ...  \n",
       "in_reply_to_screen_name                                                                                                  NaN  \n",
       "in_reply_to_status_id                                                                                                    NaN  \n",
       "lang                                                                                                                      en  \n",
       "retweet_count                                                                                                              0  \n",
       "scrap_phrase                                                                                                      #AI #human  \n",
       "scrap_time                                                                                        2019-01-19T20:28:48.670012  \n",
       "text                     researchgate stats unina naples university researchers vangeloassoluto federico book god human e...  \n",
       "tweet_id                                                                                                          1.0867e+18  \n",
       "text_raw                 #Researchgate #Stats #UNINA #Naples #University #Researchers #VanGELOAssoluto #Federico2 #book o...  \n",
       "link                                                                                                 https://t.co/UlxbL26VuL  \n",
       "RT                                                                                                                         0  \n",
       "day                                                                                                                       19  \n",
       "n_hash                                                                                                                    16  \n",
       "mentions                                                                                                                      \n",
       "n_mentions                                                                                                                 0  \n",
       "dehashed                  researchgate stats unina naples university researchers vangeloassoluto federico2 book of god & ...  \n",
       "raw_holders                   HASH HASH HASH HASH HASH HASH HASH HASH HASH WORD HASH WORD HASH HASH WORD HASH HASH HASH HASH  \n",
       "placeholders                       HASH HASH HASH HASH HASH HASH HASH HASH HASH of HASH & HASH HASH (via HASH HASH HASH HASH  \n",
       "raw_h_idx                                                                             0 1 2 3 4 5 6 7 8 10 12 13 15 16 17 18  \n",
       "raw_at_idx                                                                                                                    \n",
       "raw_w_idx                                                                                                            9 11 14  \n",
       "SANITY 1 w                                                                                                              6817  \n",
       "SANITY 1 c                                                                                                               163  \n",
       "SANITY 2 w                                                                                                              6817  \n",
       "SANITY 2 c                                                                                                               139  \n",
       "POSES                                      NOUN NOUN ADJ VERB NOUN NOUN VERB NOUN NOUN NOUN ADJ NOUN ADP NOUN NOUN NOUN NOUN  \n",
       "TAGS                                                                  NN NNS JJ VBZ NN NNS VBP NN NN NN JJ NN IN NN NN NN NN  \n",
       "DEPS                     amod nsubj compound compound compound nsubj ROOT compound dobj intj amod appos prep punct compou...  \n",
       "HEADS                    stats vangeloassoluto naples university researchers vangeloassoluto vangeloassoluto book vangelo...  \n",
       "IDXS                                                                  0 13 19 25 32 43 55 71 80 85 89 95 105 109 112 118 126  \n",
       "W_IDXS                                                                                 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning and preparation pipe\n",
    "# ----------------------------------\n",
    "\n",
    "# Retrive backup version\n",
    "df = backup.copy(deep=True)\n",
    "\n",
    "# MINI DF FOR TESTING\n",
    "#df = df.loc[:10,:].copy(deep=True)\n",
    "\n",
    "# Mesure time\n",
    "t1 = datetime.datetime.now()\n",
    "\n",
    "# Backup for raw text\n",
    "df['text_raw'] = df.text\n",
    "\n",
    "# Initial features\n",
    "df['link'], df['text'] = zip(*df.text.apply(get_and_remove_url_from_text))\n",
    "df['RT'], df['text'] = zip(*df.text.apply(get_and_remove_rt_from_text))\n",
    "df['day'] = df.created_at.apply(get_date_day)\n",
    "\n",
    "\n",
    "# Hashtag, mentions initial features\n",
    "df['hashtags'] = df.text.apply(get_hashtags)\n",
    "df['n_hash'] = df.hashtags.apply(lambda x: len(x.split()))\n",
    "df['mentions'] = df.text.apply(get_mentions)\n",
    "df['n_mentions'] = df.mentions.apply(lambda x: len(x.split()))\n",
    "\n",
    "t2 = datetime.datetime.now()\n",
    "print('Initial features, hashtag, mentions done:', t2-t1, t2-t1)\n",
    "\n",
    "# Text cleaning pipe\n",
    "df['text'] = df.text.apply(strip_html)\n",
    "df['text'] = df.text.apply(BOM_replace)\n",
    "df['text'] = df.text.apply(strip_inner_spaces)\n",
    "df['text'] = df.text.apply(lowercase_text)\n",
    "\n",
    "t3 = datetime.datetime.now()\n",
    "print('Text cleaning pipe done:', t3-t1, t3-t2)\n",
    "\n",
    "# Hash, mention strategies for text:\n",
    "df['dehashed'] = df.text.apply(hashtags_and_mentions_to_text)\n",
    "df['raw_holders'] = df.text.apply(get_raw_holders)\n",
    "df['placeholders'] = df.text.apply(get_placeholders)\n",
    "\n",
    "# Raw indexing for easy acess\n",
    "df['raw_h_idx'], df['raw_at_idx'], df['raw_w_idx'] = zip(*df.text.apply(get_hash_indexes))\n",
    "\n",
    "# Strategy pick\n",
    "df['text'] = df.text\n",
    "\n",
    "df['SANITY 1 w'] = len(df.text.str.split())\n",
    "df['SANITY 1 c'] = df.text.str.len()\n",
    "\n",
    "# Further text preparation pipe, reindexing\n",
    "df['text'] = df.text.apply(remove_stop_words)\n",
    "df['text'] = df.text.apply(remove_special_characters)\n",
    "df['text'] = df.text.apply(strip_inner_spaces)\n",
    "df['text'] = df.text.apply(lemmatize_words)\n",
    "#df['h_idx'], df['at_idx'], df['w_idx'] = zip(*df.text.apply(get_hash_indexes))\n",
    "\n",
    "# Sanity check\n",
    "df['SANITY 2 w'] = len(df.text.str.split())\n",
    "df['SANITY 2 c'] = df.text.str.len()\n",
    "\n",
    "t4 = datetime.datetime.now()\n",
    "print('Hashes strategy, prep, sanity, reindex:', t4-t1, t4-t3)\n",
    "\n",
    "# Remove special characters\n",
    "\n",
    "# Get all tags from spacy nlp at one (4-6 times faster)\n",
    "df['POSES'], df['TAGS'], df['DEPS'], df['HEADS'], df['IDXS'], df['W_IDXS'] = zip(*df.text.apply(lambda x: get_text_tokens(x)))\n",
    "\n",
    "t5 = datetime.datetime.now()\n",
    "print('Spacy tags:', t5-t1, t5-t4)\n",
    "\n",
    "df.loc[:2,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PREPARED TEXT DF TO FILE\n",
    "# -----------------------------\n",
    "\n",
    "df.to_csv('AI_Psycho_tweets_prepared.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6817, 45)\n"
     ]
    }
   ],
   "source": [
    "# RETRIVE FROM FILE\n",
    "# -----------------\n",
    "\n",
    "df = pd.read_csv('AI_Psycho_tweets_prepared.csv', sep='\\t', encoding='utf-8')\n",
    "#backup = df.copy(deep=True) # To retrive -> df = backup.copy(deep=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Twitter data + simple sentiment score CSV for further use\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob \n",
    "    \n",
    "def get_sentiment_val(text): \n",
    "    sent_value = TextBlob(text)\n",
    "    return sent_value.sentiment.polarity\n",
    "\n",
    "intresting_columns = ['text', 'hashtags', 'retweet_count', 'favorite_count']\n",
    "example_twitter_data_6k = df.loc[:, intresting_columns]\n",
    "example_twitter_data_6k['sentiment'] = example_twitter_data_6k.text.apply(get_sentiment_val)\n",
    "example_twitter_data_6k.to_csv('example_twitter_data_6k.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 107591\n",
      "unique: 12015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ai                        5155\n",
       "market                    2999\n",
       "human                     2810\n",
       "artificialintelligence    1322\n",
       "automation                1099\n",
       "bigdata                    934\n",
       "iot                        848\n",
       "machinelearning            765\n",
       "s                          757\n",
       "via                        668\n",
       "digitalmarketing           641\n",
       "intelligence               605\n",
       "socialmedia                557\n",
       "business                   536\n",
       "infographic                510\n",
       "use                        510\n",
       "artificial                 507\n",
       "rt                         506\n",
       "seo                        489\n",
       "tech                       483\n",
       "data                       458\n",
       "make                       456\n",
       "analytics                  437\n",
       "deeplearning               422\n",
       "machine                    411\n",
       "technology                 407\n",
       "smm                        406\n",
       "ml                         398\n",
       "mikequindazzi              396\n",
       "new                        382\n",
       "contentmarketing           379\n",
       "future                     374\n",
       "startup                    368\n",
       "need                       366\n",
       "datascience                361\n",
       "digital                    360\n",
       "learn                      348\n",
       "socialmediamarketing       345\n",
       "robotics                   343\n",
       "growthhacking              325\n",
       "fintech                    315\n",
       "know                       312\n",
       "cybersecurity              295\n",
       "m                          284\n",
       "help                       271\n",
       "ecommerce                  268\n",
       "b                          265\n",
       "blockchain                 264\n",
       "social                     261\n",
       "work                       244\n",
       "Name: w, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts\n",
    "\n",
    "all_words = []\n",
    "for tweet in df.text.tolist():\n",
    "    for word in tweet.split():\n",
    "        all_words.append(word)\n",
    "\n",
    "print('words:',len(all_words))\n",
    "print('unique:',len(set(all_words)))\n",
    "pd.DataFrame(all_words, columns=['w']).w.value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DF\n",
    "df = pd.read_csv('AI_Psycho_tweets_prepared.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "tf = df.loc[:, ['text','POSES']]\n",
    "\n",
    "tf[\"tl\"], tf['Tl'] = tf.text.str.count(' '), tf.POSES.str.count(' ')\n",
    "x = tf[tf['tl'] != tf['Tl']]\n",
    "\n",
    "# Delete rows with non equal numbers of words to POSes (44 of 6.8k)\n",
    "tf = tf.drop(x.index\n",
    "             \n",
    "def get_adj(TAGS):\n",
    "    tags = TAGS.split()\n",
    "    for t\n",
    "\n",
    "tf['ADJ'] = tf.TAGS.apply(get_adj)\n",
    "\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tweets avrg sentiment\n",
    "print(df.shape[0], '-', (df.sentiment.sum() / df.shape[0]))\n",
    "\n",
    "# Non neutral tweets sentiment\n",
    "noneu_tweets = df[df.sentiment != 0]\n",
    "print(noneu_tweets.shape[0], '-', (noneu_tweets.sentiment.sum() / noneu_tweets.shape[0]))\n",
    "\n",
    "# Negative tweets avrg sentiment\n",
    "neg_tweets = df[df.sentiment < 0]\n",
    "print(neg_tweets.shape, neg_tweets.sentiment.sum() / neg_tweets.shape[0])\n",
    "\n",
    "pos_tweets = df[df.sentiment > 0]\n",
    "print(pos_tweets.shape, pos_tweets.sentiment.sum() / pos_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
